# üß† Memology-ML ‚Äî AI Meme Generation Engine

[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[üá∑üá∫ –†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è](README_RU.md)

**Memology-ML** is the core machine learning component of the **Memology** project ‚Äî an AI-powered meme generation platform. This module creates **visual meme content** and **funny captions** using Stable Diffusion WebUI and Ollama (LLaMA 3.2). All processing runs **locally**, so your memes are created **privately and offline**.

## üìã Table of Contents

- [Example Memes](#-example-memes)
- [Features](#-features)
- [Architecture](#-architecture)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Setup](#-setup)
  - [Ollama Setup](#-ollama-setup)
  - [Stable Diffusion WebUI Setup](#-stable-diffusion-webui-setup)
  - [Docker Setup (recommended)](#-docker-setup-recommended)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Development](#-development)
- [Docker Guide](#-docker-guide)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)

## üé® Example Memes

| Input Idea                | Generated Meme                                               |
| ------------------------- | ------------------------------------------------------------ |
| "–∫–æ—Ç –ø—å–µ—Ç –∫–æ—Ñ–µ"           | ![Cat drinking coffee](examples/cat_with_coffee_example.png) |
| "–ª–æ—à–∞–¥—å —á–∏—Ö–Ω—É–ª–∞"          | ![Horse sneezed](examples/horse_example.png)                 |
| "–∫–æ–º–ø—å—é—Ç–µ—Ä –∏ –µ–∂—É –ø–æ–Ω—è—Ç–µ–Ω" | ![Computer obvious](examples/computer_example.png)           |

## ‚ú® Features

- üñºÔ∏è **High-quality local generation** ‚Äî All processing happens on your machine
- üé® **Multiple visual styles** ‚Äî Realistic, anime, cartoon, cyberpunk, fantasy, and more
- üß† **Smart prompt engineering** ‚Äî Automatic English prompts from Russian ideas
- üòÇ **Witty captions** ‚Äî Short, funny Russian captions generated by AI
- ‚úçÔ∏è **Automatic text overlay** ‚Äî Professional meme-style text with Impact font
- üìä **Comprehensive logging** ‚Äî Full generation process tracking
- üèóÔ∏è **Clean OOP architecture** ‚Äî Modular, testable, and extensible codebase
- ‚öôÔ∏è **Flexible configuration** ‚Äî Environment variables and config files support
- üê≥ **Docker support** ‚Äî Run locally or in containers with all dependencies
- üöÄ **FastAPI integration** ‚Äî REST API for meme generation
- üìà **Scalable** ‚Äî Multiple service instances supported

## üèóÔ∏è Architecture

The project follows modern **Object-Oriented Programming** principles with clear separation of concerns:

```
memology-ml/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/          # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ core/            # LLM and image generation abstractions
‚îÇ   ‚îú‚îÄ‚îÄ services/        # Business logic (prompts, captions, orchestration)
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Helper utilities (logging, image processing)
‚îÇ   ‚îî‚îÄ‚îÄ models/          # Data models and structures
‚îú‚îÄ‚îÄ tests/               # Unit tests
‚îú‚îÄ‚îÄ examples/            # Example memes
‚îú‚îÄ‚îÄ generated_images/    # Output directory
‚îú‚îÄ‚îÄ docker-compose.yml   # Docker Compose configuration
‚îú‚îÄ‚îÄ Dockerfile          # Docker image for API service
‚îú‚îÄ‚îÄ Dockerfile.ollama   # Docker image for ollama
‚îú‚îÄ‚îÄ main.py             # Application entry point
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îî‚îÄ‚îÄ README.md          # This file
```

### Key Components

- **ConfigManager** ‚Äî Centralized configuration with `.env` support
- **LLMClient** ‚Äî Abstraction for Ollama interactions
- **ImageGenerator** ‚Äî Stable Diffusion WebUI integration
- **PromptService** ‚Äî Visual prompt generation
- **CaptionService** ‚Äî Meme caption creation
- **MemeService** ‚Äî Main orchestration service
- **ImageUtils** ‚Äî Image manipulation and text overlay

## üì¶ Requirements

### For Local Development:

- **Python** 3.13+
- **Ollama** with LLaMA 3.2 model
- **Stable Diffusion WebUI** (AUTOMATIC1111)
- **Impact font** (`impact.ttf`)

### For Docker:

- Docker Engine 20.10+
- Docker Compose 2.0+
- Minimum 8 GB RAM (16 GB recommended for ML models)
- NVIDIA GPU (optional, for acceleration)

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/TAskMAster339/memology-ml.git
cd memology-ml
```

### 2Ô∏è‚É£ Create and activate virtual environment

```bash
# Linux / macOS
python -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

### 3Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

## üß† Setup

### ü¶ô Ollama Setup

#### Install Ollama

üëâ [https://ollama.com/download](https://ollama.com/download)

#### Pull the LLaMA 3.2 model

```bash
ollama pull llama3.2:3b
```

Verify installation:

```bash
ollama run llama3.2:3b
```

Ollama will run at: `http://localhost:11434`

### üé® Stable Diffusion WebUI Setup

#### 1Ô∏è‚É£ Install WebUI

üëâ [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

#### 2Ô∏è‚É£ Run WebUI with API enabled

```bash
python launch.py --api
```

This starts the API at: `http://127.0.0.1:7860/sdapi/v1/txt2img`

#### 3Ô∏è‚É£ (Optional) Add custom models

Download additional models from [Civitai](https://civitai.com/):

| Model            | Style                      | Link                                                        |
| ---------------- | -------------------------- | ----------------------------------------------------------- |
| **Memes XL**     | Meme-style, funny, vibrant | [Download](https://civitai.com/models/205229/memes-xl)      |
| **Crazy Horror** | Dark, surreal, horror      | [Download](https://civitai.com/models/1101129/crazy-horror) |

Place downloaded `.safetensors` files into:

```
stable-diffusion-webui/models/Stable-diffusion/
```

### üê≥ Docker Setup (recommended)

Docker Compose allows you to run all services in isolated containers with consistent environments.

#### 1Ô∏è‚É£ Configure environment variables

Create a `.env` file based on `.env.example`:

```bash
cp .env.example .env
```

Edit the `.env` file according to your requirements:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=False

# Model Configuration
OLLAMA_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_TIMEOUT=15

# Stable Diffusion Configuration
SD_BASE_URL=http://sd-webui:7860
SD_STEPS=20
SD_WIDTH=512
SD_HEIGHT=512
SD_SAMPLER=DPM++ 2M Karras
SD_CFG_SCALE=7.0

# Application Settings
OUTPUT_DIR=generated_images
LOG_FILE=generation.log
FONT_PATH=impact.ttf
```

#### 2Ô∏è‚É£ Start all services

```bash
docker compose up -d
```

This command will start all services defined in `docker-compose.yml`:

- **API Service**: available at `http://localhost:8000`
- **Swagger UI**: available at `http://localhost:8000/docs`
- **Ollama Service**: available at `http://localhost:11434`
- **Stable Diffusion WebUI**: available at `http://localhost:7860`

#### 3Ô∏è‚É£ Check service status

```bash
# View running containers
docker compose ps

# View logs from all services
docker compose logs -f

# View logs from a specific service
docker compose logs -f api
```

#### 4Ô∏è‚É£ Stop services

```bash
# Stop all services
docker compose down

# Stop and remove volumes (database and models)
docker compose down -v
```

### Ports and Application URLs

| Service          | Port  | URL                          | Description                   |
| ---------------- | ----- | ---------------------------- | ----------------------------- |
| API Service      | 8000  | http://localhost:8000        | Main API service              |
| Swagger UI       | 8000  | http://localhost:8000/docs   | Interactive API documentation |
| ReDoc            | 8000  | http://localhost:8000/redoc  | Alternative API documentation |
| Health Check     | 8000  | http://localhost:8000/health | API health status check       |
| Ollama Service   | 11434 | http://localhost:11434       | LLM model service             |
| Stable Diffusion | 7860  | http://localhost:7860        | Image generation WebUI        |

## ‚öôÔ∏è Configuration

Create a `.env` file in the project root (see `.env.example`):

```env
# Ollama Configuration
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=15
OLLAMA_BASE_URL=http://localhost:11434

# Stable Diffusion Configuration
SD_BASE_URL=http://127.0.0.1:7860
SD_STEPS=20
SD_WIDTH=512
SD_HEIGHT=512
SD_SAMPLER=DPM++ 2M Karras
SD_CFG_SCALE=7.0
SD_RESTORE_FACES=True

# Application Settings
OUTPUT_DIR=generated_images
LOG_FILE=generation.log
FONT_PATH=impact.ttf
```

### Configuration Options

| Parameter                | Description                   | Default           |
| ------------------------ | ----------------------------- | ----------------- |
| `OLLAMA_MODEL`           | LLM model name                | `llama3.2:3b`     |
| `OLLAMA_TIMEOUT`         | LLM request timeout (seconds) | `15`              |
| `SD_STEPS`               | Diffusion steps               | `20`              |
| `SD_WIDTH` / `SD_HEIGHT` | Image dimensions              | `512x512`         |
| `SD_SAMPLER`             | Sampling method               | `DPM++ 2M Karras` |
| `SD_CFG_SCALE`           | Prompt adherence strength     | `7.0`             |

## üöÄ Usage

### Basic Usage (Local)

```bash
python main.py
```

The application will generate memes for predefined examples and save them to `generated_images/`.

### Using Docker

Generate memes using FastAPI:

```bash
# Start services
docker compose up -d

# Access Swagger UI
# Open http://localhost:8000/docs in your browser

# Generate a meme via API
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–ö–æ—Ç –ø—å–µ—Ç –∫–æ—Ñ–µ",
    "style": "realistic"
  }'
```

### Programmatic Usage

```python
from src.services.meme_service import MemeService
from main import create_meme_service

# Initialize the service
meme_service = create_meme_service()

# Generate a meme
result = meme_service.generate_meme("–∫–æ—Ç –ø—å–µ—Ç –∫–æ—Ñ–µ")

if result.success:
    print(f"Meme created: {result.final_image_path}")
    print(f"Caption: {result.caption}")
else:
    print(f"Error: {result.error_message}")
```

### Custom Styles

```python
from src.models.meme import MemeStyle

# Define custom style
custom_style = MemeStyle(
    name="retro",
    description="retro 80s style, neon colors, vaporwave aesthetic"
)

# Generate with custom style
result = meme_service.generate_meme("–∫–æ—Ç –≤ –∫–æ—Å–º–æ—Å–µ", style=custom_style)
```

## üìä Logging

Each generation is logged to `logs/`:

```
2025-10-26 23:56:10 | src.services.meme_service | INFO | Starting meme generation: 9e0bbf0f
2025-10-26 23:56:22 | src.services.prompt_service | INFO | Generated prompt: A cat drinking coffee...
2025-10-26 23:56:24 | src.services.caption_service | INFO | Generated caption: –ö–æ—Ç –ø—Ä–æ—Å—Ç–æ –Ω–µ —É–º–µ–µ—Ç
2025-10-26 23:58:24 | src.services.meme_service | INFO | Meme generation completed in 134.12s
```

## üìö API Documentation

### FastAPI Endpoints

After starting with Docker, API documentation is available at:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

#### Health Check

```bash
curl http://localhost:8000/health
```

#### Generate Meme

```bash
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a meme about programming",
    "style": "classic"
  }'
```

### MemeService

**`generate_meme(user_input: str, style: Optional[MemeStyle] = None) -> MemeGenerationResult`**

Generates a meme from user input.

- **Parameters:**
  - `user_input` ‚Äî Meme idea in Russian
  - `style` ‚Äî Visual style (random if None)
- **Returns:** `MemeGenerationResult` with paths and metadata

### PromptService

**`generate_visual_prompt(user_text: str, style: MemeStyle, max_retries: int = 1) -> str`**

Creates an English visual prompt for image generation.

### CaptionService

**`generate_caption(scene_description: str) -> str`**

Generates a short, funny Russian caption.

## üß™ Testing

Run unit tests:

```bash
python -m pytest tests/
```

## üõ†Ô∏è Development

### Project Structure Philosophy

The project follows **SOLID principles** and clean architecture:

- **Single Responsibility** ‚Äî Each class has one clear purpose
- **Open/Closed** ‚Äî Open for extension, closed for modification
- **Dependency Injection** ‚Äî Dependencies passed via constructors
- **Separation of Concerns** ‚Äî Business logic separated from infrastructure

### Adding New Features

**Example: Add a new LLM provider**

1. Create a new class in `src/core/llm_client.py`:

```python
class OpenAIClient(BaseLLMClient):
    def generate(self, messages, timeout=None):
        # Implementation
        pass
```

2. Update `main.py` to use the new client:

```python
llm_client = OpenAIClient(api_key=config.openai_api_key)
```

**Example: Add a new image style**

1. Add to `src/models/meme.py`:

```python
PREDEFINED_STYLES.append(
    MemeStyle("steampunk", "steampunk art, Victorian era, brass and copper")
)
```

### Update Dependencies

```bash
# Update requirements.txt
pip freeze > requirements.txt

# Rebuild Docker image
docker compose build
```

### Code Quality Checks

```bash
# Linting
flake8 app/
black app/ --check
mypy app/

# Formatting
black app/
isort app/
```

## üê≥ Docker Guide

### Docker Compose Configuration

The project uses Docker Compose for service orchestration. Main files:

- `docker-compose.yml` ‚Äî main configuration
- `Dockerfile` ‚Äî image for API service

#### Run with production configuration:

```bash
docker compose up -d
```

#### Run with development configuration:

```bash
docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d
```

### Scaling Services

Docker Compose allows you to scale services:

```bash
# Run multiple API instances
docker compose up -d --scale api=3
```

### GPU Usage

To use GPU, add this to your `docker-compose.yml`:

```yaml
services:
  api:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

### Logging and Debugging

```bash
# View logs in real-time
docker compose logs -f api

# Enter container for debugging
docker compose exec api bash

# View resource usage
docker stats
```

### Clean Up Docker Resources

```bash
# Remove unused images, containers, and volumes
docker system prune -a --volumes

# Remove all project data
docker compose down -v
rm -rf .docker-data/
```

## üêõ Troubleshooting

### Common Issues

**Issue:** `ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=7860)`

**Solution:**

- Ensure Stable Diffusion WebUI is running with `--api` flag
- Check that WebUI is accessible at [http://127.0.0.1:7860](http://127.0.0.1:7860)

**Issue:** `ConnectionRefusedError` for Ollama

**Solution:**

- Start Ollama: `ollama serve`
- Verify model is installed: `ollama list`
- Ensure Ollama is running on correct port (11434)

**Issue:** Text doesn't fit on image

**Solution:**

- Font file missing ‚Äî ensure `impact.ttf` exists
- Reduce caption length in prompt

**Issue:** Container won't start

```bash
# Check logs
docker compose logs api

# Check container status
docker compose ps
```

**Issue:** Model won't load in Docker

```bash
# Check Ollama service availability
curl http://localhost:11434/api/tags

# Restart model service
docker compose restart ollama
```

**Issue:** Port already in use

Change ports in `.env` file or `docker-compose.yml`:

```yaml
services:
  api:
    ports:
      - "8080:8000" # Change external port
```

**Issue:** Insufficient memory

Increase limits in `docker-compose.yml`:

```yaml
services:
  api:
    deploy:
      resources:
        limits:
          memory: 8G
```

## üí° Tips & Optimization

- **Faster generation:** Reduce `SD_STEPS` to `10-15`
- **Better quality:** Use SDXL models or increase steps to `30-40`
- **Dark memes:** Try the "Crazy Horror" model
- **Anime style:** Use "Anything V5" model
- **Everything works offline** ‚Äî No API keys required!

## ü§ù Contributing

Contributions are welcome! Here's how:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following the project structure
4. Add tests for new functionality
5. Run tests: `pytest tests/`
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request to the `dev` branch

### Code Style

- Follow PEP 8 guidelines
- Use type hints
- Add docstrings to public methods
- Keep classes focused and small

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Ollama](https://ollama.com/) ‚Äî Local LLM runtime
- [AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) ‚Äî Stable Diffusion WebUI
- [Stability AI](https://stability.ai/) ‚Äî Stable Diffusion model

## üì¨ Contact

Project Link: [https://github.com/TAskMAster339/memology-ml](https://github.com/TAskMAster339/memology-ml)

---

**Built with ‚ù§Ô∏è and AI** | Made for meme enthusiasts and ML practitioners
